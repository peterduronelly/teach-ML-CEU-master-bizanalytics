---
title: "Lab week 10 - Tree-based methods"
subtitle: "Data Science and Machine Learning 2 - CEU 2018"
author: "Jeno Pal"
date: '2018-03-06'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---



```{r, message=FALSE}
library(data.table)
library(caret)
library(rpart)
library(rpart.plot)
library(xgboost)
library(randomForest)
library(gbm)
library(ISLR)
library(skimr)
```

## Bagging, random forests

We covered decision trees in DS-ML-1. Using it as a base model lets us
build many different models with less variance and better predictive power.
The downside: interpretation gets harder.

Idea: as individual trees are unstable and have high variance, train many
versions on bootstrap samples ("Bagging": Bootstrap AGGregation).
Then predict: take the average (regression),
majority vote / class share (classification). 

Random forests: randomly constrain the set of predictor variables used to
grow trees. Goal: avoid correlated trees that are very similar to each other,
still with the aim of decreasing variance.

```{r}
data(Hitters)
data <- data.table(Hitters)
skim(data)
```

```{r}
# goal: predict log salary
data <- data[!is.na(Salary)]
data[, log_salary := log(Salary)]
data[, Salary := NULL]
```


```{r}
training_ratio <- 0.75 
set.seed(1234)
train_indices <- createDataPartition(y = data[["log_salary"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

Let's see benchmarks: a linear model and a simple regression tree.
```{r}
train_control <- trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 3)

set.seed(857)
linear_model <- train(log_salary ~ .,
                      method = "lm",
                      data = data_train,
                      trControl = train_control)
linear_model
```

```{r}
set.seed(857)
simple_tree_model <- train(log_salary ~ .,
                      method = "rpart",
                      data = data_train,
                      tuneGrid = data.frame(cp = c(0.01, 0.02, 0.05)),
                      trControl = train_control)
simple_tree_model
```

```{r}
rpart.plot(simple_tree_model[["finalModel"]])
```

For random forests,
`mtry` sets the number of variables randomly chosen for a tree. When `mtry`
equals the number of features, it is the bagging.

```{r}
# random forest
set.seed(857)
rf_model <- train(log_salary ~ .,
                  method = "rf",
                  data = data_train,
                  trControl = train_control,
                  tuneGrid = data.frame(mtry = c(2, 3, 5, 7, 9, 12, 19)),
                  importance = T # to calculate variable importance measures
                  )
rf_model
```

```{r}
# the number of trees is not a tuning parameter with caret
# default is 500, you can change it with passing the parameter to train
set.seed(857)
rf_model_ntree_10 <- train(log_salary ~ .,
                  method = "rf",
                  data = data_train,
                  trControl = train_control,
                  tuneGrid = data.frame(mtry = c(2, 3, 5)),
                  ntree = 10,
                  importance = T # to calculate variable importance measures
                  )
rf_model_ntree_10
```

```{r}
# calculate test error
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))

RMSE(data_test[["log_salary"]], predict.train(rf_model, newdata = data_test))
```

(It is a small dataset hence performance measures can have large
variances.)

### Variable importance

With the ensemble models we have a hard time with interpretation.
Variable importance measures can help to see which features contribute most
to the predictive power of models. The generic `varImp` function of `caret`
does model-specific calculations, consult [here](https://topepo.github.io/caret/variable-importance.html) for a description
for your model at hand.

```{r}
varImp(rf_model)
```

```{r}
plot(varImp(rf_model))
```

## Gradient boosting machines

Gradient boosting machines: also ensembles of trees, however,
the method of choosing them is different. Idea: get the residual and train
next tree to predict (explain) the residual. Then add it to the previous
trees, with a shrinkage parameter (to avoid overfitting).

Another difference: GBMs use shallow trees (controlled by
`interaction.depth`) whereas RFs use unpruned, large trees (with low bias
and high variance). Common part: idea of bagging.

```{r}
gbm_grid <- expand.grid(n.trees = c(100, 500, 1000), 
                        interaction.depth = c(2, 3, 5), 
                        shrinkage = c(0.005, 0.01, 0.1),
                        n.minobsinnode = c(5))
  
gbm_model <- train(log_salary ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid,
                   verbose = FALSE # gbm by default prints too much output
                   )
gbm_model
```

4 hyperparameters: [the curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) 
can make it hard to come up
with efficient grids for hyperparameter search. However, with `gbm` and many
other models, in reality the models to be estimated are not exponentially
growing with the number of hyperparameters (see [here](https://topepo.github.io/caret/random-hyperparameter-search.html)).

```{r}
plot(varImp(gbm_model))
```

Lower `eta` means slower learning, hence more trees are necessary to have
good performance.

Not tuned: `bag.fraction` parameter (set to default 0.5): for the construction
of each tree, only `bag.fraction` share of the sample is used (randomly
selected, see `?gbm`). 
This, again, is the same idea as with bagging: decrease
variance of the model. You can pass another value for it via
giving `train` an argument `bag.fraction`, just like we saw with
`rf` and `ntree`.

```{r}
gbm_grid_2 <- data.frame(n.trees = c(1000), 
                         interaction.depth = c(5), 
                         shrinkage = c(0.005),
                         n.minobsinnode = c(5))
  
gbm_model_2 <- train(log_salary ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid_2,
                   bag.fraction = 0.8,
                   verbose = FALSE # gbm by default prints too much output
                   )
gbm_model_2
```


### XGBoost

A celebrated implementation of the gradient boosting idea. 
_"Both xgboost and gbm follows the principle of gradient boosting. There are however, the difference in modeling details. Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance."_


See documentation [here](http://xgboost.readthedocs.io/).
It proved to be very stable and widely applicable. For the many hyperparameters,
consult the documentation.
```{r}
xgbGrid <- expand.grid(nrounds = c(500, 1000),
                       max_depth = c(2, 3, 5),
                       eta = c(0.01, 0.05),
                       gamma = 0,
                       colsample_bytree = c(0.5, 0.7),
                       min_child_weight = 1,
                       subsample = c(0.5))
set.seed(857)
xgboost_model <- train(log_salary ~ .,
                       method = "xgbTree",
                       data = data_train,
                       trControl = train_control,
                       tuneGrid = xgbGrid)
xgboost_model
```

```{r}
plot(varImp(xgboost_model))
```


```{r}
resamples_object <- resamples(list("rpart" = simple_tree_model,
                                   "rf" = rf_model,
                                   "gbm" = gbm_model,
                                   "xgboost" = xgboost_model))
summary(resamples_object)
```




